{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare text for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>Or anyone else have noticed the fact that firs...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>This movie is so unreal. French movies like th...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>UK discriminated against Roma\\n\\nThe governmen...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>Caddyshack 2 has a dreadful reputation, due on...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>This is a small film , few characters ,theatri...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     topic\n",
       "6121  Or anyone else have noticed the fact that firs...     movie\n",
       "4775  This movie is so unreal. French movies like th...     movie\n",
       "1347  UK discriminated against Roma\\n\\nThe governmen...  politics\n",
       "6243  Caddyshack 2 has a dreadful reputation, due on...     movie\n",
       "6444  This is a small film , few characters ,theatri...     movie"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/posts_info.csv', index_col=0)[['text','topic']]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK economy facing major risks The UK manufacturing sector will continue to face serious challenges over the next two years, the British Chamber of Commerce (BCC) has said. The groups quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced major risks and warned that growth is set to slow.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "df_ = df[['text', 'topic']].copy()\n",
    "\n",
    "for i, txt in enumerate(df_.text.values):\n",
    "    txt = textwrap.shorten(txt, width=512, fix_sentence_endings=True).rsplit('.  ')\n",
    "    txt.pop() if len(txt)>1 else None\n",
    "    txt = '. '.join(txt)+'.'\n",
    "    df_.text.values[i] = txt\n",
    "    \n",
    "df_.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK economy facing major risks The UK manufacturing sector will continue to face serious challenges over the next two years, the British Chamber of Commerce (BCC) has said. The groups quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced major risks and warned that growth is set to slow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['without economy supply facing most major risks the uk manufacturing sector will still begin to gradually face serious government challenges throughout the next two years, the previous british chamber to trade ( bcc ) has said. so the economic information quarterly survey of 2004 found exports had picked up in all in last three months per annum 2004 to achieve best levels of in three years. the higher rise came however despite exchange inflation being highly cited as as a serious major concern. moreover, the census found the whole uk business still faced major threat rates when people warned while investment growth will set just a slow.',\n",
       " 'with economy facing with major risks the british manufacturing sector had to continue to gradually face serious market challenges had throughout approximately the very next two decade, the uk chamber of accounts ( bcc ) had stated. the july 2004 quarterly round of companies journal found exports only had picked a up decline around the core uk just three months after 2004 to achieve their best levels in london towns. significant rise came despite exchange rates being cited as a main concern. however, the fortune magazine found the potential whole uk industry economy still faced just major risks quickly and warned that investment is set to slow.',\n",
       " 'uk economy has major impacts the uk manufacturing sector will continue slowly to slowly face serious challenges over the next two months, studies the british chamber of industrial commerce ( london bae ) has earlier denied. the current quarterly reports of june 2003 uk growth had picked up decline in the previous three decade as of 2004 linked to their best values via eight cycles. prices would rise began grow despite increased rates not being cited as having a very major concern. again, reports bcc 2016 found the whole uk economy might still had faced many major risks and warned that growth is set out to slow.',\n",
       " 'although uk economy showed growing risks the uk manufacturing sector may continue to pose serious business challenges over the entire next two three years, the official british department of commerce ( bae ) has observed. the first 2007 quarterly data map of companies found sales revenue had picked rise up in this last three months as of 2004 to their net best levels declined in eight consecutive years. today the economic rise followed despite exchange rates being cited as still under global concern. however, research the group found the whole northern uk community still faced various significant threats here and concluded that growth is starting to slow.',\n",
       " 'uk industrial economy has considering additional economic risks the uk manufacturing sector has will continue to not face significant challenges over the next ten industry years, like the latest british chamber of consumer commerce ( bp ) has initially said. the following groups did a survey that companies found exports have had picked up for fifty the last three months of 2003 improved to their best levels in eight years. probably worst examples came despite exchange rates that cited by a major boost. however, the former bcc found the whole uk economy still along with british manufacturers who had felt that growth further is set to slow.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug = naf.Sequential([\n",
    "    naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-multilingual-uncased', action=\"substitute\", aug_p=.4, aug_max=20, device='cuda'\n",
    "    ),\n",
    "        naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-multilingual-uncased', action=\"insert\", aug_p=.4, aug_max=20, device='cuda'\n",
    "    ),\n",
    "])\n",
    "print(df_.text[0])\n",
    "aug.augment(df_.text[0], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>IAAF to rule on Greek sprint pair Greek sprint...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>Years have gone by since Don Wilson used his m...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5065</th>\n",
       "      <td>Was the script more fitting for a 30 minute si...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>I have to say that some of the other reviews o...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>Super union merger plan touted Two of Britains...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     topic\n",
       "2319  IAAF to rule on Greek sprint pair Greek sprint...     sport\n",
       "4352  Years have gone by since Don Wilson used his m...     movie\n",
       "5065  Was the script more fitting for a 30 minute si...     movie\n",
       "5450  I have to say that some of the other reviews o...     movie\n",
       "1562  Super union merger plan touted Two of Britains...  politics"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_.sample(5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame(columns=['text','topic'])\n",
    "\n",
    "for i, txt in zip(sample.index, sample.text.values):\n",
    "    generated = pd.DataFrame({'text': aug.augment(txt, n=10)})\n",
    "    generated['topic'] = sample.at[i, 'topic']\n",
    "\n",
    "    newdf = pd.concat([newdf, generated])#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used kaggle GPU to generate augmented dataset (70230,2). Let's use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/augtxt.csv')\n",
    "df_full = pd.read_csv('../data/augtxt_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's map text to embedding\n",
    "from transformers import AutoTokenizer\n",
    "# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "    assert model_name in ['bert_cased','bert_uncased', 'roberta', 'distilbert',]\n",
    "\n",
    "    checkpoint_names = {\n",
    "        'bert_cased': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n",
    "        'bert_uncased': 'bert-base-uncased',  # https://huggingface.co/bert-base-uncased\n",
    "\n",
    "    }\n",
    "\n",
    "    model_classes = {\n",
    "        'bert_cased': BertModel,\n",
    "        'bert_uncased': BertModel,\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        AutoTokenizer.from_pretrained(checkpoint_names[model_name]),\n",
    "        model_classes[model_name].from_pretrained(checkpoint_names[model_name])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            max_length=512,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.texts['input_ids'][idx], 'attention_mask': self.texts['attention_mask'][idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts['input_ids'])\n",
    "\n",
    "    # def LayerNormaliz\n",
    "\n",
    "def reload(values, tokenizer):\n",
    "    dataset = PostDataset(values.tolist(), tokenizer)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=32,\n",
    "                        collate_fn=data_collator, pin_memory=True, shuffle=False)\n",
    "\n",
    "    return (\n",
    "        dataset, data_collator, loader\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings_labels(model, loader):  \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    " \n",
    "    total_embeddings = []\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        batch = {key: batch[key].to(device)\n",
    "                 for key in ['attention_mask', 'input_ids']}\n",
    "\n",
    "        embeddings = model(**batch)['last_hidden_state'][:, :25, :] #as in paper\n",
    "\n",
    "        total_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = get_model('bert_uncased')\n",
    "\n",
    "dataset, data_collator, loader = reload(df['text'].values, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f756f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [43:24<00:00,  1.19s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70230, 25, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings_labels(model, loader)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_by_tokens(tensor:torch.Tensor, method:str):\n",
    "    assert method in ['max_pooling', 'mean_pooling']\n",
    "    _dict = {\n",
    "        'max_pooling' : lambda tensor: torch.max(tensor, dim=1)[0],\n",
    "        'mean_pooling' : lambda tensor: torch.mean(tensor, dim=1)\n",
    "    }\n",
    "    return _dict[method](tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(array, method):\n",
    "    assert method in ['identity', 'standard', 'layer', 'minmax']\n",
    "    axis = 0\n",
    "    _dict = {\n",
    "        'identity' : lambda x: x,\n",
    "        'standard' : lambda x: x / np.linalg.norm(x, axis=0),\n",
    "        'layer' : lambda x: (x - x.mean(axis)) / x.std(axis),\n",
    "        'minmax' : lambda x: (x - x.min(axis)) / (x.max(axis) - x.min(axis))\n",
    "    }\n",
    "    return _dict[method](array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_pooling__identity', 'max_pooling__standard', 'max_pooling__layer', 'max_pooling__minmax', 'mean_pooling__identity', 'mean_pooling__standard', 'mean_pooling__layer', 'mean_pooling__minmax'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdict = {}\n",
    "for pooling in ['max_pooling', 'mean_pooling']:\n",
    "    array = pool_by_tokens(embeddings, pooling)\n",
    "    for norm_method in ['identity', 'standard', 'layer', 'minmax' ]:\n",
    "        embdict[pooling+'__'+norm_method] = normalization(array, norm_method)\n",
    "\n",
    "np.save('./embeddings/embaug_dict.npy', embdict)\n",
    "embdict.keys()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {topic: i for i, topic in enumerate(pd.Series(df.topic.values).value_counts().index.sort_values())}\n",
    "\n",
    "txt_labels = df.topic.values\n",
    "num_labels = np.vectorize(map_dict.get)(df.topic.values)\n",
    "\n",
    "np.save('./embeddings/embaug_txt_labels.npy', txt_labels)\n",
    "np.save('./embeddings/embaug_num_labels.npy', num_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13a6bab5aa7a42c9de985e2bd7ef091596b396c4d5464faf56dce47736f84832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
