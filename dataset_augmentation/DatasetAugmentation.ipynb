{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Safety alert as GM recalls cars\\n\\nThe worlds ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>It seems whenever a mainstream film company wa...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>Gangsters dominate gaming chart\\n\\nVideo games...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>If you like silly comedies like Airplane youll...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>This serial is interesting to watch as an MST3...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     topic\n",
       "437   Safety alert as GM recalls cars\\n\\nThe worlds ...  business\n",
       "5335  It seems whenever a mainstream film company wa...     movie\n",
       "2522  Gangsters dominate gaming chart\\n\\nVideo games...      tech\n",
       "4478  If you like silly comedies like Airplane youll...     movie\n",
       "5909  This serial is interesting to watch as an MST3...     movie"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../posts_info.csv', index_col=0)[['text','topic']]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK economy facing major risks The UK manufacturing sector will continue to face serious challenges over the next two years, the British Chamber of Commerce (BCC) has said. The groups quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced major risks and warned that growth is set to slow.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "df_ = df[['text', 'topic']].copy()\n",
    "\n",
    "for i, txt in enumerate(df_.text.values):\n",
    "    txt = textwrap.shorten(txt, width=512, fix_sentence_endings=True).rsplit('.  ')\n",
    "    txt.pop() if len(txt)>1 else None\n",
    "    txt = '. '.join(txt)+'.'\n",
    "    df_.text.values[i] = txt\n",
    "    \n",
    "df_.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK economy facing major risks The UK manufacturing sector will continue to face serious challenges over the next two years, the British Chamber of Commerce (BCC) has said. The groups quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced major risks and warned that growth is set to slow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['uk economy facing international risks as uk coal manufacturing sector sector will find its best face facing challenges into the next two years, the 2007 british conservative chamber of commerce ( bcc ) study has said. by the groups following quarterly survey industry companies found exports they had largely picked up in the relatively last three months of 2004 prior to their high positions in eight years. the economic rise followed from where reduced rates was cited like a central public concern. in however, industry groups it found during the 2006 economy uk economy still faced major global risks and warned that food is set to fare.',\n",
       " 'uk is economy facing major risks because the uk small manufacturing sector sector is now continue being a face global financial challenges again over the next two final quarters, the small british government of commerce ( fas ) uk has then argued. the groups s quarterly survey identified companies changed consumers not already picked up in the last three months of estate 2004 up to their best levels in previous years. the crisis remains despite exchange rates being cited as not a major concern. similarly, the survey found the new uk economy might still faced major market risks and warned trade trade is set off to fall.',\n",
       " 'uk economy facing major commercial risks and uk financial sector must want not to completely face serious challenges under our next two decades, said the british chamber without commerce ( bcc ) has subsequently cited. the same 2011 quarterly report in companies report found exports themselves had picked forth in the following two preceding decades as 2004 to their best levels in eight previous years. the further worst came despite exchange value rates still being long cited as a longer major concern. however, this uk observer found only the whole uk economy still faced major risks abroad and have warned consumers that growth is set to slow.',\n",
       " 'uk economy facing facing major challenges the central uk manufacturing systems will continue to suffer serious challenges over they next 10 years, the latest british congressional chamber to of commerce ( bcc ) global magazine said. the groups quarterly survey study of uk countries found american exports had picked up in its last three final months to begin 2004 thanks to their highest scores in eight minutes. thus an unexpected rise came after competitive exchange data being highly cited was a rather major threat. however, the bcc says the whole uk economy will take faced major risks and warned that european commerce is set forward to slow.',\n",
       " 'thus uk economy suffered great financial risks because the large uk manufacturing sector does continue to often face economic challenges ever over the coming next two years, according that british chamber by agriculture ( bcc ) has said. the 2009 quarterly survey the companies found from exports it had picked fast out in the two last two final months of 2004 to their best levels in previous three years. high rise came despite exchange rates being cited as a leading concern. also, if bcc found the now whole uk manufacturing economy worse and it faced major risks is warned for growth production is set relatively very slow.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug = naf.Sequential([\n",
    "    naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-multilingual-uncased', action=\"substitute\", aug_p=.4, aug_max=20, device='cuda'\n",
    "    ),\n",
    "        naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-multilingual-uncased', action=\"insert\", aug_p=.4, aug_max=20, device='cuda'\n",
    "    ),\n",
    "])\n",
    "print(df_.text[0])\n",
    "aug.augment(df_.text[0], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>Three DJs replace Peel radio show The late Joh...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>I bought the DVD of this movie because I am a ...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>With all this death around, I’ve found myself ...</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>The king is dead long live the King!  The tria...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>Blair backs pre-election budget Tony Blair has...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          topic\n",
       "927   Three DJs replace Peel radio show The late Joh...  entertainment\n",
       "6941  I bought the DVD of this movie because I am a ...          movie\n",
       "3681  With all this death around, I’ve found myself ...          covid\n",
       "5771  The king is dead long live the King!  The tria...          movie\n",
       "1419  Blair backs pre-election budget Tony Blair has...       politics"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_.sample(5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame(columns=['text','topic'])\n",
    "\n",
    "for i, txt in zip(sample.index, sample.text.values):\n",
    "    generated = pd.DataFrame({'text': aug.augment(txt, n=10)})\n",
    "    generated['topic'] = sample.at[i, 'topic']\n",
    "\n",
    "    newdf = pd.concat([newdf, generated])#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('newdf.csv')\n",
    "df_full = pd.read_csv('newdf_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's map text to embedding\n",
    "from transformers import AutoTokenizer\n",
    "# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "    assert model_name in ['bert_cased','bert_uncased', 'roberta', 'distilbert',]\n",
    "\n",
    "    checkpoint_names = {\n",
    "        'bert_cased': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n",
    "        'bert_uncased': 'bert-base-uncased',  # https://huggingface.co/bert-base-uncased\n",
    "\n",
    "    }\n",
    "\n",
    "    model_classes = {\n",
    "        'bert_cased': BertModel,\n",
    "        'bert_uncased': BertModel,\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        AutoTokenizer.from_pretrained(checkpoint_names[model_name]),\n",
    "        model_classes[model_name].from_pretrained(checkpoint_names[model_name])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            max_length=512,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.texts['input_ids'][idx], 'attention_mask': self.texts['attention_mask'][idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts['input_ids'])\n",
    "\n",
    "    # def LayerNormaliz\n",
    "\n",
    "def reload(values, tokenizer):\n",
    "    dataset = PostDataset(values.tolist(), tokenizer)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=32,\n",
    "                        collate_fn=data_collator, pin_memory=True, shuffle=False)\n",
    "\n",
    "    return (\n",
    "        dataset, data_collator, loader\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings_labels(model, loader):  \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    " \n",
    "    total_embeddings = []\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        batch = {key: batch[key].to(device)\n",
    "                 for key in ['attention_mask', 'input_ids']}\n",
    "\n",
    "        embeddings = model(**batch)['last_hidden_state'][:, :25, :] #as in paper\n",
    "\n",
    "        total_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = get_model('bert_uncased')\n",
    "\n",
    "dataset, data_collator, loader = reload(df['text'].values, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f756f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [43:24<00:00,  1.19s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70230, 25, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings_labels(model, loader)\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_by_tokens(tensor:torch.Tensor, method:str):\n",
    "    assert method in ['max_pooling', 'mean_pooling']\n",
    "    _dict = {\n",
    "        'max_pooling' : lambda tensor: torch.max(tensor, dim=1)[0],\n",
    "        'mean_pooling' : lambda tensor: torch.mean(tensor, dim=1)\n",
    "    }\n",
    "    return _dict[method](tensor).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(array, method):\n",
    "    assert method in ['identity', 'standard', 'layer', 'minmax']\n",
    "    axis = 0\n",
    "    _dict = {\n",
    "        'identity' : lambda x: x,\n",
    "        'standard' : lambda x: x / np.linalg.norm(x, axis=0),\n",
    "        'layer' : lambda x: (x - x.mean(axis)) / x.std(axis),\n",
    "        'minmax' : lambda x: (x - x.min(axis)) / (x.max(axis) - x.min(axis))\n",
    "    }\n",
    "    return _dict[method](array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_pooling__identity', 'max_pooling__standard', 'max_pooling__layer', 'max_pooling__minmax', 'mean_pooling__identity', 'mean_pooling__standard', 'mean_pooling__layer', 'mean_pooling__minmax'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdict = {}\n",
    "for pooling in ['max_pooling', 'mean_pooling']:\n",
    "    array = pool_by_tokens(embeddings, pooling)\n",
    "    for norm_method in ['identity', 'standard', 'layer', 'minmax' ]:\n",
    "        embdict[pooling+'__'+norm_method] = normalization(array, norm_method)\n",
    "\n",
    "np.save('./embeddings/embaug_dict.npy', embdict)\n",
    "embdict.keys()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {topic: i for i, topic in enumerate(pd.Series(df.topic.values).value_counts().index.sort_values())}\n",
    "\n",
    "txt_labels = df.topic.values\n",
    "num_labels = np.vectorize(map_dict.get)(df.topic.values)\n",
    "\n",
    "np.save('./embeddings/embaug_txt_labels.npy', txt_labels)\n",
    "np.save('./embeddings/embaug_num_labels.npy', num_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13a6bab5aa7a42c9de985e2bd7ef091596b396c4d5464faf56dce47736f84832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
